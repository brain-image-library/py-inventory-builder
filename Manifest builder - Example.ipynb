{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c5aca1-dfaa-4767-9271-91b49a84f0e8",
   "metadata": {},
   "source": [
    "# Manifest builder - Example `/bil/data/39/19/39194b133512dab0/P10_JL0176/STPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62538a8e-1e4a-49c2-a96a-f18d814c3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os.path\n",
    "import glob\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import fnmatch\n",
    "import tabulate\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "import hashlib\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import uuid\n",
    "import shutil\n",
    "from datetime import date\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from warnings import resetwarnings, warn as warning\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True,nb_workers=5)\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c3357-e7c5-4542-a11e-c6619ded5e08",
   "metadata": {},
   "source": [
    "# Load file with files on directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beade464-0558-4b3b-8945-1b6ee300db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/bil/data/39/19/39194b133512dab0/P10_JL0176/STPT'\n",
    "\n",
    "if Path(directory).exists():\n",
    "    exit\n",
    "\n",
    "file = directory.replace('/', '_')\n",
    "output_filename = file + '.tsv'\n",
    "\n",
    "if Path('/scratch/icaoberg/'+output_filename).exists():\n",
    "    shutil.copyfile('/scratch/icaoberg/' + output_filename, output_filename)\n",
    "\n",
    "if Path(output_filename).exists():\n",
    "    df = pd.read_csv(output_filename, sep='\\t', low_memory=False)\n",
    "else:\n",
    "    files = [x for x in Path(directory).glob('**/*') if x.is_file()]\n",
    "    df = pd.DataFrame()\n",
    "    df['fullpath']= files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0e062-10a4-4e98-b13e-39233261e558",
   "metadata": {},
   "source": [
    "## Get file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234f32c-ad02-4a73-8ed8-249c059ec5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_extension(filename):\n",
    "    if Path(filename).is_file():\n",
    "        extension = Path(filename).suffix\n",
    "        if extension == '.tiff' or extension == '.tif':\n",
    "            if str(filename).find('ome.tif') > 0:\n",
    "                extension = '.ome.tif'\n",
    "\n",
    "        return extension\n",
    "\n",
    "if 'extension' not in df.keys():\n",
    "    df['extension'] = df['fullpath'].parallel_apply(get_file_extension)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b3612-2f13-4353-a992-1ba99003b1b6",
   "metadata": {},
   "source": [
    "## Get filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af87778-e58b-4baa-bd22-a3fe8da59f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(filename):\n",
    "        return Path(filename).stem + Path(filename).suffix\n",
    "    \n",
    "if 'filename' not in df.keys():\n",
    "    df['filename'] = df['fullpath'].parallel_apply(get_filename)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9271e0-e9b1-48a3-bb6f-2f6a1ed0ae5d",
   "metadata": {},
   "source": [
    "## Get file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addac75-7ca9-4c47-8f0a-a283b97c4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filetype( extension ):\n",
    "        images = {'.tiff', '.png', '.tif', '.ome.tif', '.jpeg', '.gif', '.ome.tiff', 'jpg', '.jp2'}\n",
    "        if extension in images:\n",
    "            return 'images'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "if 'filetype' not in df.keys():\n",
    "    df['filetype'] = df['extension'].parallel_apply(get_filetype)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288ec8b-dd36-4e6f-9e03-358f1ebcf153",
   "metadata": {},
   "source": [
    "## Get file creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5d845-f169-41eb-af9f-13986e1b117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_creation_date(filename):\n",
    "    t = os.path.getmtime(str(filename))\n",
    "    return str(datetime.datetime.fromtimestamp(t))\n",
    "\n",
    "if 'modification_time' not in df.keys():\n",
    "    df['modification_time'] = df['fullpath'].parallel_apply(get_file_creation_date)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4911f6-6c97-4281-b1f1-94beaf575985",
   "metadata": {},
   "source": [
    "## Get file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0f0eb-5d88-4c46-a0d3-502818ea3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size(filename):\n",
    "    return Path(filename).stat().st_size\n",
    "\n",
    "if 'size' not in df.keys():\n",
    "    df['size'] = df['fullpath'].parallel_apply(get_file_size)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6930d737-ea4c-49c5-9d0d-481871bc36ad",
   "metadata": {},
   "source": [
    "## Get mime-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27012c3f-2cc2-421a-ba9c-a0d65dc09653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "\n",
    "def get_mime_type(filename):\n",
    "    mime = magic.Magic(mime=True)\n",
    "    return mime.from_file(filename)\n",
    "\n",
    "\n",
    "if 'mime-type' not in df.keys():\n",
    "    df['mime-type'] = df['fullpath'].parallel_apply(get_mime_type)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d48baf-653e-4b06-bd76-1f746999dfd0",
   "metadata": {},
   "source": [
    "## Get download link for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba86d2-c686-4e4d-9e03-b5e270c5fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(filename):\n",
    "    filename = str(filename)\n",
    "    return filename.replace('/bil/data/','https://download.brainimagelibrary.org/')\n",
    "\n",
    "if 'download_url' not in df.keys():\n",
    "    df['download_url'] = df['fullpath'].parallel_apply(get_url)\n",
    "    df.to_csv( output_filename, sep='\\t', index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57822030-09a6-4ca4-950b-17e0855c9218",
   "metadata": {},
   "source": [
    "## Compute md5 checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba15dbd-cfd0-4e27-9571-2e7444c8822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def compute_md5sum(filename):\n",
    "    # BUF_SIZE is totally arbitrary, change for your app!\n",
    "    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!\n",
    "\n",
    "    md5 = hashlib.md5()\n",
    "\n",
    "    if Path(filename).is_file():\n",
    "        with open(filename, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(BUF_SIZE)\n",
    "                if not data:\n",
    "                    break\n",
    "                md5.update(data)\n",
    "\n",
    "    return md5.hexdigest()\n",
    "\n",
    "def __update_dataframe(dataset, chunk):\n",
    "    for index, datum in chunk.iterrows():\n",
    "        dataset.loc[index,'md5'] = chunk.loc[index,'md5']\n",
    "        \n",
    "def __get_chunk_size(dataframe):\n",
    "    if len(dataframe) < 1000:\n",
    "        return 10\n",
    "    elif len(dataframe) < 10000:\n",
    "        return 250\n",
    "    elif len(dataframe) < 100000:\n",
    "        return 100\n",
    "    elif len(dataframe) < 500000:\n",
    "        return 5000\n",
    "    else:\n",
    "        return 1000\n",
    "    \n",
    "if len(df) < 100:\n",
    "    df['md5'] = df['fullpath'].parallel_apply(compute_md5sum)\n",
    "    df.to_csv(output_filename, sep='\\t', index=False)\n",
    "else:\n",
    "    if 'md5' in df.keys():\n",
    "        files = df[df['md5'].isnull()]\n",
    "    else:\n",
    "        files = df\n",
    "    \n",
    "    if len(files) != 0:\n",
    "        n = __get_chunk_size(files)\n",
    "        print(f'Number of files to process is {str(len(files))}')\n",
    "        chunks = np.array_split(files, n)\n",
    "\n",
    "        chunk_counter = 1\n",
    "        for chunk in chunks:\n",
    "            print(f'Processing chunk {str(chunk_counter)} of {str(len(chunks))}')\n",
    "            chunk['md5'] = chunk['fullpath'].parallel_apply(compute_md5sum)\n",
    "            __update_dataframe(df, chunk)\n",
    "            chunk_counter = chunk_counter + 1\n",
    "\n",
    "            if chunk_counter % 25 == 0 or chunk_counter == len(chunks):\n",
    "                print('Saving chunks to disk')\n",
    "                df.to_csv('/scratch/icaoberg/' + output_filename, sep='\\t', index=False)\n",
    "    else:\n",
    "        print('No files left to process')\n",
    "                \n",
    "if Path('/scratch/icaoberg/'+output_filename).exists():\n",
    "    shutil.copyfile('/scratch/icaoberg/' + output_filename, output_filename)\n",
    "    Path('/scratch/icaoberg/'+output_filename).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c69d55-0cba-4d9a-ad68-92c2192785de",
   "metadata": {},
   "source": [
    "## Computer sh256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be745ff5-5a12-41ee-a87c-f98e23d0185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def compute_sha256sum(filename):\n",
    "    # BUF_SIZE is totally arbitrary, change for your app!\n",
    "    BUF_SIZE = 65536  # lets read stuff in 64kb chunks!\n",
    "\n",
    "    sha256 = hashlib.sha256()\n",
    "    if Path(filename).is_file():\n",
    "        with open(filename, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(BUF_SIZE)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha256.update(data)\n",
    "\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def __update_dataframe(dataset, chunk):\n",
    "    for index, datum in chunk.iterrows():\n",
    "        dataset.loc[index,'sha256'] = chunk.loc[index,'sha256']\n",
    "        \n",
    "def __get_chunk_size(dataframe):\n",
    "    if len(dataframe) < 1000:\n",
    "        return 10\n",
    "    elif len(dataframe) < 10000:\n",
    "        return 100\n",
    "    elif len(dataframe) < 100000:\n",
    "        return 1000\n",
    "    elif len(dataframe) < 500000:\n",
    "        return 5000\n",
    "    else:\n",
    "        return 1000\n",
    "    \n",
    "if len(df) < 100:\n",
    "    df['sha256'] = df['fullpath'].parallel_apply(compute_sha256sum)\n",
    "    df.to_csv(output_filename, sep='\\t', index=False)\n",
    "else:\n",
    "    if 'sha256' in df.keys():\n",
    "        files = df[df['sha256'].isnull()]\n",
    "    else:\n",
    "        files = df\n",
    "    \n",
    "    if len(files) != 0:\n",
    "        n = __get_chunk_size(files)\n",
    "        print(f'Number of files to process is {str(len(files))}')\n",
    "        chunks = np.array_split(files, n)\n",
    "\n",
    "        chunk_counter = 1\n",
    "        for chunk in chunks:\n",
    "            print(f'Processing chunk {str(chunk_counter)} of {str(len(chunks))}')\n",
    "            chunk['sha256'] = chunk['fullpath'].parallel_apply(compute_sha256sum)\n",
    "            __update_dataframe(df, chunk)\n",
    "            chunk_counter = chunk_counter + 1\n",
    "\n",
    "            if chunk_counter % 25 == 0 or chunk_counter == len(chunks):\n",
    "                print('Saving chunks to disk')\n",
    "                df.to_csv('/scratch/icaoberg/' + output_filename, sep='\\t', index=False)\n",
    "    else:\n",
    "        print('No files left to process')\n",
    "                \n",
    "if Path('/scratch/icaoberg/'+output_filename).exists():\n",
    "    shutil.copyfile('/scratch/icaoberg/' + output_filename, output_filename)\n",
    "    Path('/scratch/icaoberg/'+output_filename).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684eab8-58a3-44cb-98b3-668effc64f9f",
   "metadata": {},
   "source": [
    "## Compute image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19137edb-d402-43b4-8eaa-d6ddc875e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_image_size( filename ):\n",
    "    if Path(filename).exists():\n",
    "        img = Image.open(filename)\n",
    "        return img.size\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def __update_dataframe(dataset, chunk):\n",
    "    for index, datum in chunk.iterrows():\n",
    "        dataset.loc[index,'dimensions'] = chunk.loc[index,'dimensions']\n",
    "        \n",
    "def __get_chunk_size(dataframe):\n",
    "    if len(dataframe) < 1000:\n",
    "        return 10\n",
    "    elif len(dataframe) < 10000:\n",
    "        return 100\n",
    "    elif len(dataframe) < 100000:\n",
    "        return 1000\n",
    "    elif len(dataframe) < 500000:\n",
    "        return 5000\n",
    "    else:\n",
    "        return 1000\n",
    "    \n",
    "if len(df) < 100:\n",
    "    df['dimensions'] = df['fullpath'].parallel_apply(get_image_size)\n",
    "    df.to_csv(output_filename, sep='\\t', index=False)\n",
    "else:\n",
    "    if 'dimensions' in df.keys():\n",
    "        files = df[df['filetype']=='image']\n",
    "        files = df[df['dimensions'].isnull()]\n",
    "    else:\n",
    "        files = df\n",
    "        files = df[df['filetype']=='image']\n",
    "    \n",
    "    if len(files) != 0:\n",
    "        n = __get_chunk_size(files)\n",
    "        print(f'Number of files to process is {str(len(files))}')\n",
    "        chunks = np.array_split(files, n)\n",
    "\n",
    "        chunk_counter = 1\n",
    "        for chunk in chunks:\n",
    "            print(f'Processing chunk {str(chunk_counter)} of {str(len(chunks))}')\n",
    "            chunk['dimensions'] = chunk['fullpath'].parallel_apply(get_image_size)\n",
    "            __update_dataframe(df, chunk)\n",
    "            chunk_counter = chunk_counter + 1\n",
    "\n",
    "            if chunk_counter % 25 == 0 or chunk_counter == len(chunks):\n",
    "                print('Saving chunks to disk')\n",
    "                df.to_csv('/scratch/icaoberg/' + output_filename, sep='\\t', index=False)\n",
    "    else:\n",
    "        print('No files left to process')\n",
    "                \n",
    "if Path('/scratch/icaoberg/'+output_filename).exists():\n",
    "    shutil.copyfile('/scratch/icaoberg/' + output_filename, output_filename)\n",
    "    Path('/scratch/icaoberg/'+output_filename).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2be23-bf75-4129-b4a6-6cedf1e172de",
   "metadata": {},
   "source": [
    "# Compute dataset level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f1ce8-f567-43f0-a8da-c6763575e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "def get_url(filename):\n",
    "    return filename.replace('/bil/data/','https://download.brainimagelibrary.org/')\n",
    "\n",
    "from numpyencoder import NumpyEncoder\n",
    "\n",
    "def generate_dataset_uuid(directory):\n",
    "    if directory[-1] == '/':\n",
    "        directory = directory[:-1]\n",
    "    \n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb827cb6-940c-440f-8cd9-dc29c1c00b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['dataset_uuid'] = generate_dataset_uuid(directory)\n",
    "dataset['directory'] = file.replace('_','/')\n",
    "dataset['download_url'] = get_url(dataset['directory'])\n",
    "dataset['number_of_files'] = len(df)\n",
    "dataset['size'] = df['size'].sum()\n",
    "dataset['pretty_size'] = humanize.naturalsize(dataset['size'], gnu=True)\n",
    "dataset['frequencies'] = df['extension'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abbb65-3282-4a9c-a6ba-65c1e7162b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fullpath'] = df['fullpath'].astype(str)\n",
    "files = df.to_dict('records')\n",
    "dataset['manifest'] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231994b-619a-41cb-a404-d2bb4fb8f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = generate_dataset_uuid(directory) + '.json'\n",
    "with open(output_filename, \"w\") as ofile:\n",
    "   json.dump(dataset, ofile, indent=4, sort_keys=True, ensure_ascii=False, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e24637-ace4-4e0b-994c-25c38daae036",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(output_filename).exists():\n",
    "    if not Path('.data').exists():\n",
    "        Path('.data').mkdir()\n",
    "        \n",
    "    tfile = directory.replace('/', '_') + '.tsv'\n",
    "    if Path(tfile).exists():\n",
    "        shutil.move(tfile, '.data/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
